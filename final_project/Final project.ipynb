{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of poi = 17, nb of empl. = 141. Ratio = 12.06 %\n",
      "\n",
      "{'classifier': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
      "           weights='distance'),\n",
      " 'classifier__n_neighbors': 4,\n",
      " 'classifier__weights': 'distance',\n",
      " 'reduce_dim': SelectKBest(k=14, score_func=<function f_classif at 0x000000000C2829E8>),\n",
      " 'reduce_dim__k': 14}\n",
      "\n",
      "('F1 macro =', 0.5885198301145145)\n",
      "Total predictions:   71\t\n",
      "True positives:    2\tFalse positives:    3\n",
      "False negatives:    7\tTrue negatives:   59\n",
      "\n",
      "Accuracy: 0.85915\tPrecision: 0.40000\tRecall: 0.22222\n",
      "F1: 0.28571\tF2: 0.24390\n",
      "\n",
      "time: 8.799 s\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hood\\anaconda3\\envs\\py2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "import math\n",
    "import operator\n",
    "import pickle\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, GenericUnivariateSelect\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "#sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "    \"\"\"\n",
    "    \n",
    "    fraction = 0.\n",
    "    \n",
    "    poi_messages = float(poi_messages)\n",
    "    all_messages = float(all_messages)\n",
    "    \n",
    "    if isnan(poi_messages) or poi_messages == 0:\n",
    "        fraction = 0.\n",
    "    else: fraction = poi_messages / all_messages\n",
    "    \n",
    "    return round(fraction, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "\n",
    "features_list = [\n",
    "    'poi',\n",
    "    'salary',\n",
    "    'to_messages',\n",
    "    'deferral_payments',\n",
    "    'total_payments',\n",
    "    'exercised_stock_options',\n",
    "    'bonus',\n",
    "    'restricted_stock',\n",
    "    'shared_receipt_with_poi',\n",
    "    'restricted_stock_deferred',\n",
    "    'total_stock_value',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'from_messages',\n",
    "    'other',\n",
    "    'from_this_person_to_poi',\n",
    "    'director_fees',\n",
    "    'deferred_income',\n",
    "    'long_term_incentive',\n",
    "    #'email_address',\n",
    "    'from_poi_to_this_person',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "### KPI\n",
    "\n",
    "\n",
    "\n",
    "# print \"Total people:\", len(data_dict)\n",
    "# print\n",
    "\n",
    "poi = set()\n",
    "for empl in data_dict:\n",
    "    if data_dict[empl]['poi'] == True:poi.add(empl)\n",
    "\n",
    "# print \"Total POI = {} / non-POI = {}\".format(len(poi), (len(data_dict) - len(poi)))\n",
    "# print \"Ratio POI = {} % / non-POI = {} %\".format(\\\n",
    "#     round((float(len(poi))/len(data_dict))*100, 2) , round((float((len(data_dict) - len(poi)))/len(data_dict))*100, 2))\n",
    "# print\n",
    "\n",
    "# print \"Total label + features:\", len(data_dict['LAY KENNETH L'])\n",
    "# feature_nan_dict = dict()\n",
    "# for _, values in data_dict.items():\n",
    "#     for key, value in values.items():\n",
    "#         if isnan(value):\n",
    "#             if key not in feature_nan_dict:\n",
    "#                 feature_nan_dict[key] = 1\n",
    "#             else: feature_nan_dict[key] += 1\n",
    "# print \"Quantity of NaN per feature:\"\n",
    "# pp(feature_nan_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 2: Remove outliers & errors\n",
    "\n",
    "\n",
    "\n",
    "#Before\n",
    "# print \"nb of poi = {}, nb of empl. = {}. Ratio = {} %\".format(len(poi), len(data_dict),\\\n",
    "#                                                             round( float( len(poi)) / len(data_dict), 4)*100 )\n",
    "\n",
    "\n",
    "\n",
    "#Removing persona with low amount of data\n",
    "ratio_high_NaN = 0.85\n",
    "high_NaN = dict()\n",
    "for name, values in data_dict.items():\n",
    "    i = 0\n",
    "    for _, value in values.items():\n",
    "        if isnan(value): i += 1\n",
    "    if (i / 21.) >= ratio_high_NaN: high_NaN[name] = i / 21.\n",
    "# pp(sorted(high_NaN.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "#Removing outliers\n",
    "outliers = ('LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK')\n",
    "for k in outliers:\n",
    "    data_dict.pop(k, None)\n",
    "\n",
    "\n",
    "\n",
    "#Replacing NaN values by 0\n",
    "for _, values in data_dict.items():\n",
    "    for key, value in values.items():\n",
    "        if isnan(value):\n",
    "            values[key] = 0\n",
    "pd_data = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "\n",
    "\n",
    "#Removing Statistical outlier (https://en.wikipedia.org/wiki/Interquartile_range)\n",
    "\n",
    "# Computing IQR using Pandas functionnality\n",
    "Q1 = pd_data.quantile(0.25)\n",
    "Q3 = pd_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "series_outl = [pd_data[(pd_data>(Q3 + 1.5*IQR) ) | (pd_data<(Q1 - 1.5*IQR) )].count(axis=1), pd_data['poi']]\n",
    "series_outl[0].name = \"bad_feats\"\n",
    "outl = pd.concat(series_outl, axis=1)\n",
    "outl.sort_values(by='bad_feats', ascending=False, inplace=True)\n",
    "# print outl.head(8)\n",
    "\n",
    "#Removing outliers\n",
    "outliers = ('LAY KENNETH L', 'TOTAL', 'FREVERT MARK A')\n",
    "for k in outliers:\n",
    "    data_dict.pop(k, None)\n",
    "\n",
    "\n",
    "\n",
    "#After\n",
    "poi2 = set()\n",
    "for empl in data_dict:\n",
    "    if data_dict[empl]['poi'] == True:poi2.add(empl)\n",
    "print \"nb of poi = {}, nb of empl. = {}. Ratio = {} %\".format(len(poi2), len(data_dict),\\\n",
    "                                                            round( float( len(poi2)) / len(data_dict), 4)*100 )\n",
    "\n",
    "\n",
    "\n",
    "#Removing errors\n",
    "\n",
    "#Check sums\n",
    "payment_sum = [\n",
    "    'salary',\n",
    "    'deferral_payments',\n",
    "    'bonus',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'other',\n",
    "    'director_fees',\n",
    "    'deferred_income',\n",
    "    'long_term_incentive',\n",
    "]\n",
    "\n",
    "stock_sum = [\n",
    "    'exercised_stock_options',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "]\n",
    "\n",
    "delta_pay = pd_data[payment_sum].sum(axis=1) != pd_data['total_payments']\n",
    "delta_stock = pd_data[stock_sum].sum(axis=1) != pd_data['total_stock_value']\n",
    "\n",
    "# print list(delta_pay[delta_pay==True].index), list(delta_stock[delta_stock==True].index)\n",
    "\n",
    "\n",
    "#Corrections\n",
    "data_dict['BELFER ROBERT']['deferral_payments'] = 0\n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options'] = 0\n",
    "data_dict['BELFER ROBERT']['restricted_stock'] = 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred'] = -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 0\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['deferred_income'] = -102500\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments'] = 137864\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options'] = 15456290\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock'] = 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred'] = -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value'] = 15456290\n",
    "data_dict['BHATNAGAR SANJAY']['expenses'] = 137864\n",
    "data_dict['BHATNAGAR SANJAY']['other'] = 0\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees'] = 0\n",
    "\n",
    "\n",
    "pd_data = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "delta_pay = pd_data[payment_sum].sum(axis=1) != pd_data['total_payments']\n",
    "delta_stock = pd_data[stock_sum].sum(axis=1) != pd_data['total_stock_value']\n",
    "\n",
    "# print list(delta_pay[delta_pay==True].index), list(delta_pay[delta_stock==True].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 3: Create features\n",
    "\n",
    "\n",
    "#New features (basic ratios)\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "    \n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "\n",
    "features_list.append('fraction_from_poi')\n",
    "features_list.append('fraction_to_poi')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "np.set_printoptions(suppress=True,\n",
    "   formatter={'float_kind':'{:16.0f}'.format}, linewidth=130)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Correlation Matrix with Heatmap (from https://towardsdatascience.com/)\n",
    "pd_data = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "#pd_data = pd.DataFrame(data=features)\n",
    "\n",
    "corrmat = pd_data.corr()\n",
    "top_corr_features = corrmat.index\n",
    "# plt.figure(figsize=(20,20))\n",
    "# g=sns.heatmap(pd_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Splitting\n",
    "###Replacing train_test_split by StratifiedShuffleSplit\n",
    "###To have stratified & shuffled split \n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits = 5,\\\n",
    "    test_size = 0.5,\\\n",
    "    random_state = 42,\\\n",
    ")\n",
    "\n",
    "for train_index, test_index in sss.split(features, labels): \n",
    "    features_train = list()\n",
    "    features_test  = list()\n",
    "    labels_train   = list()\n",
    "    labels_test    = list()\n",
    "    for ii in train_index:\n",
    "        features_train.append( features[ii] )\n",
    "        labels_train.append( labels[ii] )\n",
    "    for jj in test_index:\n",
    "        features_test.append( features[jj] )\n",
    "        labels_test.append( labels[jj] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html        \n",
    "  \n",
    "#Provided to give you a starting point. Try a variety of classifiers.\n",
    "#Integrated in the pipeline\n",
    "\n",
    "#Univariate feature selector with configurable strategy\n",
    "#Integrated in the pipeline\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "search_space = [\n",
    "    \n",
    "    \n",
    "    #KNC\n",
    "    {'reduce_dim': [PCA()],\n",
    "     'reduce_dim__n_components': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "     \n",
    "     'classifier': [KNeighborsClassifier()],\n",
    "     'classifier__n_neighbors': [2, 3, 4, 5, 6],\n",
    "     'classifier__weights': ['uniform', 'distance']},\n",
    "    \n",
    "    {'reduce_dim': [SelectKBest(f_classif)],\n",
    "     'reduce_dim__k': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "     \n",
    "     'classifier': [KNeighborsClassifier()],\n",
    "     'classifier__n_neighbors': [2, 3, 4, 5, 6],\n",
    "     'classifier__weights': ['uniform', 'distance']},\n",
    "    \n",
    "    \n",
    "    #AdaBoost\n",
    "#     {'reduce_dim': [PCA()],\n",
    "#      'reduce_dim__n_components': [2, 8, 14, 20],\n",
    "     \n",
    "#      'classifier': [AdaBoostClassifier()],\n",
    "#      'classifier__n_estimators': [10, 50, 100],\n",
    "#      'classifier__learning_rate': [0.1, 0.5, 1.0]},\n",
    "    \n",
    "#     {'reduce_dim': [SelectKBest(f_classif)],\n",
    "#      'reduce_dim__k': [2, 8, 14, 20],\n",
    "     \n",
    "#      'classifier': [AdaBoostClassifier()],\n",
    "#      'classifier__n_estimators': [10, 50, 100],\n",
    "#      'classifier__learning_rate': [0.1, 0.5, 1.0]},\n",
    "    \n",
    "    \n",
    "    #SVC\n",
    "#      {'reduce_dim': [PCA()],\n",
    "#       'reduce_dim__n_components': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "     \n",
    "#      'classifier': [SVC()],\n",
    "#      'classifier__C': [1, 10, 100, 1000, 10000],\n",
    "#      'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#      'classifier__class_weight' :[None, 'balanced']},\n",
    "    \n",
    "#      {'reduce_dim': [SelectKBest(f_classif)],\n",
    "#       'reduce_dim__k': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "     \n",
    "#      'classifier': [SVC()],\n",
    "#      'classifier__C': [1, 10 ,100 ,1000, 10000],\n",
    "#      'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#      'classifier__class_weight' :[None, 'balanced']},\n",
    "    \n",
    "    \n",
    "     #Decision Tree\n",
    "#     {'reduce_dim': [PCA()],\n",
    "#      'reduce_dim__n_components': [2, 5, 10, 15, 20],\n",
    "     \n",
    "#      'classifier': [DecisionTreeClassifier()],\n",
    "#      'classifier__criterion': ['gini', 'entropy'],\n",
    "#      'classifier__splitter': ['best', 'random'],\n",
    "#      'classifier__max_depth': [None, 5, 10, 15],\n",
    "#      'classifier__min_samples_split': [2, 5, 10, 15, 20],\n",
    "#      'classifier__max_features': [None, 'sqrt', 'log2', 'auto'],\n",
    "#      'classifier__max_leaf_nodes' : [None, 25, 50, 100, 1000]},\n",
    "    \n",
    "#     {'reduce_dim': [SelectKBest(f_classif)],\n",
    "#      'reduce_dim__k': [2, 5, 10, 15, 19, 20, 21],\n",
    "     \n",
    "#      'classifier': [DecisionTreeClassifier()],\n",
    "#      'classifier__criterion': ['gini', 'entropy'],\n",
    "#      'classifier__splitter': ['best', 'random'],\n",
    "#      'classifier__max_depth': [None, 5, 10, 15],\n",
    "#      'classifier__min_samples_split': [2, 5, 10, 15, 20],\n",
    "#      'classifier__max_features': [None, 'sqrt', 'log2', 'auto'],\n",
    "#      'classifier__max_leaf_nodes' : [None, 25, 50, 100, 1000],\n",
    "#     },\n",
    "    \n",
    "    \n",
    "    #Random Forest\n",
    "#     {'reduce_dim': [PCA()],\n",
    "#      'reduce_dim__n_components': [2, 4, 5, 6, 10, 18],\n",
    "     \n",
    "#      'classifier': [RandomForestClassifier()],\n",
    "#      'classifier__n_estimators': [10, 100, 1000],\n",
    "#      'classifier__criterion': ['gini', 'entropy']},\n",
    "    \n",
    "#     {'reduce_dim': [SelectKBest(f_classif)],\n",
    "#      'reduce_dim__k': [2, 5, 9, 10, 11, 15, 19],\n",
    "     \n",
    "#      'classifier': [RandomForestClassifier()],\n",
    "#      'classifier__n_estimators': [10, 100, 1000],\n",
    "#      'classifier__criterion': ['gini', 'entropy']},\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "clf = GridSearchCV(pipe, search_space, scoring='f1_macro')\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "predictions = clf.predict(features_test)\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "print \"\"\n",
    "pp (clf.best_params_)\n",
    "print \"\"\n",
    "print ('F1 macro =', clf.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "\n",
    "for prediction, truth in zip(predictions, labels_test):\n",
    "    if prediction == 0 and truth == 0:\n",
    "        true_negatives += 1\n",
    "    elif prediction == 0 and truth == 1:\n",
    "        false_negatives += 1\n",
    "    elif prediction == 1 and truth == 0:\n",
    "        false_positives += 1\n",
    "    elif prediction == 1 and truth == 1:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "        print \"All predictions should take value 0 or 1.\"\n",
    "        print \"Evaluating performance for processed predictions:\"\n",
    "        break\n",
    "\n",
    "\n",
    "RESULTS_FORMAT_STRING = \"Total predictions: {:4d}\\t\\nTrue positives: {:4d}\\tFalse positives: {:4d}\\n\\\n",
    "False negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "PERF_FORMAT_STRING = \"\\nAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\nF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "\n",
    "total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "\n",
    "try:\n",
    "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "    print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "    print \"\"\n",
    "except:\n",
    "    print classification_report(labels_test, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"time:\", round(t1-t0, 3), \"s\"\n",
    "print \"done\"\n",
    "\n",
    "#input(\"Truc machin lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
